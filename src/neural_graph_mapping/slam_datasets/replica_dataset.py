"""Dataset class for Replica dataset."""
import json
import pathlib
import re
from typing import Optional

import numpy as np
import open3d as o3d
import PIL.Image
import torch
import yoco
from tqdm import tqdm
import trimesh

from neural_graph_mapping import camera, slam_dataset

_ocv2ogl = torch.tensor(
    [
        [1.0, 0.0, 0.0, 0.0],
        [0.0, -1.0, 0.0, 0.0],
        [0.0, 0.0, -1.0, 0.0],
        [0.0, 0.0, 0.0, 1.0],
    ]
)


class ReplicaDataset(slam_dataset.SLAMDataset):
    """Replica dataset class.

    Trajectories were originally generated by Edgar Sucar, et al. for iMAP: Implicit Mapping
    and Positioning in Real-Time.

    They are available for download at https://cvg-data.inf.ethz.ch/nice-slam/data/Replica.zip

    Expected directory format:
        {root_dir}/cam_params.json
        {root_dir}/{scene}/traj.txt
        {root_dir}/{scene}/results/frame001704.jpg
        {root_dir}/{scene}/results/depth001704.png
        {root_dir}/{scene}_mesh.ply
        ...

    Attributes:
        camera (camera.Camera):
            The camera information for the images.
    """

    class Config(slam_dataset.SLAMDataset.Config, total=False):
        """Configuration dictionary for ReplicaDataset.

        Attributes:
            camera:
                Camera parameters of the dataset. Will be passed as kwargs to
                constructor of camera.Camera.
            fps:
                Frames per second used to calculate timestamp in sequence mode.
            frame_skip:
                Number of frames skipped between consecutive frames.
                In ray mode this can be used to reduce memory usage of dataset, and
                shouldn't decrease performance when images are very close together.
                In sequence mode fps and frame_skip together define speed of camera
                motion.  For example, both, changing fps 30->60 and frame_skip 0->1
                double the sequence speed, but only in the second case the information
                of every other frame is lost.
                If 0 all frames are used, 1 every other, 2 every third, ...
            scale:
                Scaling factor for depth and poses.
            prefetch:
                Whether to prefetch the whole dataset (i.e., store in memory).
        """

        camera: dict
        fps: int
        frame_skip: int
        scale: float
        prefetch: bool

    default_config: Config = {
        "fps": 30,
        "frame_skip": 0,
        "scale": 1,
        "prefetch": False,
    }

    def __init__(
        self,
        config: Config,
    ) -> None:
        """Initialize the dataset.

        Args:
            config:
                Configuration dictionary of dataset. Provided dictionary will be merged
                with default_dict. See ReplicaDataset.Config for supported keys.
        """
        self.config = yoco.load_config(config, current_dict=ReplicaDataset.default_config)
        super().__init__(self.config)

        # set up paths
        self._image_dir_path = self.scene_dir_path / "results"
        self._poses_file = self.scene_dir_path / "traj.txt"

        self._num_images = (
            len(list(self._image_dir_path.iterdir())) // 2
        )  # divide by 2, because there are color and depth image in the same directory

        self.camera = self._load_camera()
        self.gt_c2ws = self._load_gt_c2ws()

    def _parse_config(self) -> None:
        """Parse configuration dictionary into member variables."""
        super()._parse_config()
        self.scene = self.config["scene"]
        self._scale = self.config["scale"]
        self._prefetch = self.config["prefetch"]
        self._fps = self.config["fps"]
        self._frame_skip = self.config["frame_skip"]

    @staticmethod
    def get_available_scenes(root_dir: str) -> list[str]:
        """Return available scenes at the given root directory."""
        root_dir_path = pathlib.Path(root_dir)
        scene_dir_paths = [p for p in root_dir_path.iterdir() if p.is_dir()]
        valid_scene_dir_paths = [p for p in scene_dir_paths if (p / "traj.txt").is_file()]
        scene_names = [p.name for p in valid_scene_dir_paths]
        return scene_names

    @property
    def num_images(self) -> int:
        """Return number of images in this dataset."""
        return self._num_images

    @property
    def scene_dir_path(self) -> pathlib.Path:
        """Return directory for the current scene."""
        return self.root_dir_path / self.scene

    @property
    def has_gt_mesh(self) -> bool:
        """Return whether dataset has a ground-truth mesh and it exists."""
        return self.gt_mesh_path.is_file()

    @property
    def gt_mesh_path(self) -> pathlib.Path:
        """Return path of ground-truth mesh.

        Should only be called and / or implemented when has_gt_mesh is True.
        """
        mesh_file_name = f"{self.scene}_mesh.ply"
        mesh_file_path: pathlib.Path = self.root_dir_path / mesh_file_name
        return mesh_file_path

    def load_gt_mesh(self) -> slam_dataset.Mesh:
        """Return ground-truth mesh or None if not available for this dataset.

        Should only be called and / or implemented when has_gt_mesh is True.
        """
        # NOTE Replica ground-truth are quadmeshes, which open3d can't load, so we use trimesh
        # to load the mesh and then convert to open3d from there
        tm_mesh: trimesh.Trimesh = trimesh.load(self.gt_mesh_path)
        return slam_dataset.Mesh(tm_mesh.as_open3d)

    def _load_camera(self) -> camera.Camera:
        intrinsic_path = self.root_dir_path / "cam_params.json"
        with open(intrinsic_path) as f:
            camera_data = json.load(f)["camera"]
        self._depth_scale = camera_data["scale"]
        return camera.Camera(
            camera_data["w"],
            camera_data["h"],
            camera_data["fx"],
            camera_data["fy"],
            camera_data["cx"],
            camera_data["cy"],
            pixel_center=0.0,
        )

    def set_mode(self, mode: Optional[str]) -> None:
        """See slam_dataset.SLAMDataset.set_mode."""
        if mode is None or mode == self._mode:
            return

        # reset current data
        self._rgbds = self._times = self._ijs = self._frame_ids = None

        self._mode = mode

        if self._mode == "ray":
            self._load_dataset = self._load_ray_dataset
            self._get_item = self._get_ray_item
            self._get_len = self._get_num_rays
        elif self._mode == "sequence":
            self._load_dataset = self._load_sequence_dataset
            self._get_item = self._get_sequence_item
            self._get_len = self._get_num_images
        else:
            raise ValueError("Dataset mode must be ray or sequence.")

        if self._prefetch:
            self._load_dataset()

    def _load_dataset(self) -> None:
        """Load dataset into memory.

        See ReplicaDataset._load_ray_dataset and ReplicaDataset._load_sequence_dataset for
        details.
        """
        raise NotImplementedError(
            "_load_dataset should point to _load_ray_dataset or _load_sequence_dataset"
        )

    def _load_gt_c2ws(self) -> torch.Tensor:
        gt_c2ws = torch.from_numpy(
            np.loadtxt(self._poses_file).reshape(-1, 4, 4)[:: self._frame_skip + 1]
        ).float()  # N, 4, 4
        gt_c2ws[:, :3, 3] *= self._scale
        gt_c2ws @= _ocv2ogl
        return gt_c2ws.to(self.device)

    def _load_ray_dataset(self) -> None:
        """Load ray dataset into memory."""
        image_file_paths = sorted(list(self._image_dir_path.glob("frame*")))
        depth_file_paths = sorted(list(self._image_dir_path.glob("depth*")))
        image_file_paths = image_file_paths[:: self._frame_skip + 1]
        depth_file_paths = depth_file_paths[:: self._frame_skip + 1]

        rgb_sample = self._load_color(image_file_paths[0])
        w = rgb_sample.shape[1]
        h = rgb_sample.shape[0]
        n = len(image_file_paths)

        self._ijs = torch.cartesian_prod(torch.arange(h), torch.arange(w))
        self._rgbds = torch.empty(
            len(image_file_paths), h, w, 4, device=self.device
        )  # N, H, W, 4
        self._frame_ids = torch.empty(
            len(image_file_paths), h, w, dtype=torch.long, device=self.device
        )

        for i, (image_file_path, depth_file_path) in tqdm(
            enumerate(zip(image_file_paths, depth_file_paths)),
            total=len(image_file_paths),
            desc="Loading dataset",
            dynamic_ncols=True,
        ):
            self._rgbds[i] = self._load_rgbd(image_file_path, depth_file_path)
            self._frame_ids[i] = i

        # repeat ijs, memory inefficient, but good enough here
        self._ijs = self._ijs.repeat(n, 1)

        # flatten to rays
        self._rgbds = self._rgbds.reshape(-1, 4)
        self._frame_ids = self._frame_ids.reshape(-1)

        self.print_memory_usage()

    def _load_sequence_dataset(self) -> None:
        """Load sequence dataset into memory."""
        image_file_paths = sorted(list(self._image_dir_path.glob("frame*")))
        depth_file_paths = sorted(list(self._image_dir_path.glob("depth*")))
        image_file_paths = image_file_paths[:: self._frame_skip + 1]
        depth_file_paths = depth_file_paths[:: self._frame_skip + 1]

        rgb_sample = self._load_color(image_file_paths[0])
        width = rgb_sample.shape[1]
        height = rgb_sample.shape[0]

        self._times = []
        self._rgbds = torch.empty(len(image_file_paths), height, width, 4, device=self.device)

        seconds_per_frame = 1 / self._fps

        for i, (image_file_path, depth_file_path) in tqdm(
            enumerate(zip(image_file_paths, depth_file_paths)),
            total=len(image_file_paths),
            desc="Loading dataset",
            dynamic_ncols=True,
        ):
            self._times.append(i * seconds_per_frame)
            self._rgbds[i] = self._load_rgbd(image_file_path, depth_file_path)

        self._times = torch.Tensor(self._times).to(self.device)

        self.print_memory_usage()

    def __getitem__(self, index: int) -> dict:
        """Return a sample of the dataset.

        See ReplicaDataset._get_ray_item and ReplicaDataset._get_sequence_item for details.
        """
        return self._get_item(index)

    def _get_item(self, index: int) -> dict:
        raise NotImplementedError(
            "_get_item should point to _get_ray_item or _get_sequence_item"
        )

    def _get_ray_item(self, index: int) -> dict:
        if self._prefetch:
            return {
                "ij": self._ijs[index],
                "rgbd": self._rgbds[index],
                "c2w": self.gt_c2ws[self._frame_ids[index]],
            }
        else:
            return self._load_ray_item(index)

    def _load_ray_item(self, index: int) -> dict:
        raise NotImplementedError()

    def _get_sequence_item(self, index: int) -> dict:
        if self._prefetch:
            return {
                "time": self._times[index],
                "rgbd": self._rgbds[index],
                "c2w": self.gt_c2ws[index],
            }
        else:
            return self._load_sequence_item(index)

    def _load_sequence_item(self, index: int) -> dict:
        image_file_paths = sorted(list(self._image_dir_path.glob("frame*")))
        depth_file_paths = sorted(list(self._image_dir_path.glob("depth*")))
        image_file_paths = image_file_paths[:: self._frame_skip + 1]
        depth_file_paths = depth_file_paths[:: self._frame_skip + 1]

        c2ws = torch.from_numpy(
            np.loadtxt(self._poses_file).reshape(-1, 4, 4)[:: self._frame_skip + 1]
        ).float()
        c2ws[:, :3, 3] *= self._scale
        c2ws @= _ocv2ogl
        c2ws = c2ws.to(self.device)

        seconds_per_frame = 1 / self._fps

        image_file_path = image_file_paths[index]
        depth_file_path = depth_file_paths[index]
        rgbd = self._load_rgbd(image_file_path, depth_file_path)
        rgbd = rgbd.to(self.device)

        time = torch.tensor(index * seconds_per_frame, device=self.device)

        return {"time": time, "rgbd": rgbd, "c2w": c2ws[index]}

    def _load_rgbd(self, color_path: pathlib.Path, depth_path: pathlib.Path) -> torch.Tensor:
        """Load RGB-D image from filepath.

        Args:
            color_path: Filepath of color image.
            depth_path: Filepath of depth image.

        Returns:
            Tensor containing RGB-D image. RGBD. Shape (H,W,4).
            First three channels are RGB, 0-1.  Last channel is depth in meters.
        """
        rgb = self._load_color(color_path)
        depth = self._load_depth(depth_path)
        return torch.cat([rgb, depth[..., None]], dim=-1)

    def _load_color(self, color_path: pathlib.Path) -> torch.Tensor:
        """Load color image from filepath.

        Args:
            color_path: Filepath of color image.

        Returns:
            Tensor containing color image. RGB. 0-1. Shape (H,W,3).
        """
        color = torch.from_numpy(
            np.asarray(PIL.Image.open(color_path), dtype=np.float32) / 255
        ).to(self.device)
        return color

    def _load_depth(self, depth_path: pathlib.Path) -> torch.Tensor:
        """Load depth image from filepath.

        Args:
            depth_path: Filepath of depth image.

        Returns:
            Tensor containing depth image. Depth. Meters. Shape (H,W).
        """
        depth = torch.from_numpy(
            np.asarray(PIL.Image.open(depth_path), dtype=np.float32)
            / self._depth_scale
            * self._scale
        ).to(self.device)
        return depth

    def _get_id(self, path: str) -> int:
        """Return last integer found in path."""
        breakpoint()
        return int(re.findall(r"\d+", path)[-1])

    def __len__(self) -> int:
        """Return number of samples in dataset.

        For mode ray this is the total number of pixels.
        For mode sequence
        """
        return self._get_len()

    def _get_len(self) -> int:
        raise NotImplementedError("_get_len should point to _get_num_rays or _get_num_images")

    def _get_num_images(self) -> int:
        return self._num_images

    def _get_num_rays(self) -> int:
        return self._num_images * self.camera.width * self.camera.height

    def print_memory_usage(self) -> None:
        """Print memory usage of dataset."""
        if self._mode == "ray":
            mem = (
                self._ijs.element_size() * self._ijs.numel()
                + self._frame_ids.element_size() * self._frame_ids.numel()
                + self._rgbds.element_size() * self._rgbds.numel()
                + self.gt_c2ws.element_size() * self.gt_c2ws.numel()
            )
        elif self._mode == "sequence":
            mem = (
                self._rgbds.element_size() * self._rgbds.numel()
                + self.gt_c2ws.element_size() * self.gt_c2ws.numel()
                + self._times.element_size() * self._times.numel()
            )
        else:
            raise ValueError("ReplicaDataset mode must be ray or sequence.")

        print("Dataset memory usage: ", mem / 1e9, "GB")

    @property
    def custom_scene_bounds(self) -> Optional[torch.Tensor]:
        """Return scene bounds as used by CO-SLAM and NICE-SLAM.

        Returns:
            None if scene bounds are not known for the scene.
            Otherwise array of shape (2, 3). First row containing minimum point, last row
            containing maximum point.
        """
        if self.scene == "room0":
            return torch.tensor([[-1.0, 7.0], [-1.3, 3.7], [-1.7, 1.4]]).T
        elif self.scene == "room1":
            return torch.tensor([[-5.6, 1.4], [-3.2, 2.8], [-1.6, 1.8]]).T
        elif self.scene == "room2":
            return torch.tensor([[-0.9, 6.0], [-3.3, 1.8], [-3.0, 0.7]]).T
        elif self.scene == "office0":
            return torch.tensor([[-2.2, 2.6], [-3.4, 2.1], [-1.4, 2.0]]).T
        elif self.scene == "office1":
            return torch.tensor([[-1.9, 3.1], [-1.6, 2.6], [-1.1, 1.8]]).T
        elif self.scene == "office2":
            return torch.tensor([[-3.5, 3.1], [-2.9, 5.4], [-1.3, 1.6]]).T
        elif self.scene == "office3":
            return torch.tensor([[-5.2, 3.6], [-6.0, 3.3], [-1.3, 1.9]]).T
        elif self.scene == "office4":
            return torch.tensor([[-1.3, 5.4], [-2.4, 4.3], [-1.3, 1.7]]).T
        else:
            return None
